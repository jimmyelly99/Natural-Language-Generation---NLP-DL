{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word-Level Neural Language Model and Use it to Generate Text\n",
    "\n",
    "A language model that can predict the probability of next word in the sequence, based on the word already observed in the sequence\n",
    "\n",
    "You will discover how to develop a statistical language model using deep learning in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article,\n",
    "How to prepare text for developing a word-based language model.\n",
    "How to design and fit a neural language model with a learned embedding and an LSTM hidden layer.\n",
    "How to use the learned language model to generate new text with similar statistical properties as the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load text into memory\n",
    "def load_doc(filename):\n",
    "    #open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    #read all text\n",
    "    text = file.read()\n",
    "    #close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTRODUCTION AND ANALYSIS.\n",
      "The Republic of Plato is the longest of his works with the exception of the Laws, and is certainly the greatest of them. There are nearer approaches to modern metaphysics in the Philebus and in the Sophist; the Politicus or Statesman is more ideal; the form and institutions of the State are more clearly drawn out in the Laws; as works of art, the Symposium and the Protagoras are of higher excellence. But no other Dialogue of Plato has the same largeness of view and the\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\republic_book_input.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean text\n",
    "\n",
    "# Replace ‘–‘ with a white space so we can split words better.\n",
    "# Split words based on white space.\n",
    "# Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
    "# Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
    "# Normalize all words to lowercase to reduce the vocabulary size.\n",
    "\n",
    "import string\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphanumeric\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'and', 'analysis', 'the', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'them', 'there', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn', 'out', 'in', 'the', 'laws', 'as', 'works', 'of', 'art', 'the', 'symposium', 'and', 'the', 'protagoras', 'are', 'of', 'higher', 'excellence', 'but', 'no', 'other', 'dialogue', 'of', 'plato', 'has', 'the', 'same', 'largeness', 'of', 'view', 'and', 'the', 'same', 'perfection', 'of', 'style', 'no', 'other', 'shows', 'an', 'equal', 'knowledge', 'of', 'the', 'world', 'or', 'contains', 'more', 'of', 'those', 'thoughts', 'which', 'are', 'new', 'as', 'well', 'as', 'old', 'and', 'not', 'of', 'one', 'age', 'only', 'but', 'of', 'all', 'nowhere', 'in', 'plato', 'is', 'there', 'a', 'deeper', 'irony', 'or', 'a', 'greater', 'wealth', 'of', 'humour', 'or', 'imagery', 'or', 'more', 'dramatic', 'power', 'nor', 'in', 'any', 'other', 'of', 'his', 'writings', 'is', 'the', 'attempt', 'made', 'to', 'interweave', 'life', 'and', 'speculation', 'or', 'to', 'connect', 'politics', 'with', 'philosophy', 'the', 'republic', 'is', 'the', 'centre', 'around', 'which', 'the', 'other', 'dialogues', 'may', 'be', 'grouped', 'here', 'philosophy', 'reaches', 'the', 'highest', 'point', 'cp', 'especially', 'in', 'books', 'v', 'vi', 'vii', 'to', 'which', 'ancient', 'thinkers', 'ever', 'attained', 'plato', 'among']\n",
      "Total Tokens: 214970\n",
      "Unique Tokens: 10409 \n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d ' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequence: 214919\n"
     ]
    }
   ],
   "source": [
    "# save clean text \n",
    "\n",
    "# We can organize the long list of tokens into sequences of 50 input words and 1 output word.\n",
    "# That is, sequences of 51 words.\n",
    "# We can do this by iterating over the list of tokens from token 51 onwards and \n",
    "# taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.\n",
    "# We will transform the tokens into space-separated strings for later storage in a file.\n",
    "# The code to split the list of clean tokens into sequences with a length of 51 tokens is listed below.\n",
    "\n",
    "# organize into sequence of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequence: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we can save the sequences to a new file for later loading.\n",
    "# We can define a new function for saving lines of text to a file. \n",
    "# This new function is called save_doc() and is listed below. It takes as input a list of lines and a filename\n",
    "\n",
    "# save tokens to file\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will train is a neural language model. It has a few unique characteristics:\n",
    "\n",
    "It uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
    "It learns the representation at the same time as learning the model.\n",
    "It learns to predict the probability for the next word using the context of the last 100 words.\n",
    "Specifically, we will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "\n",
    "# We can load our training data using the load_doc() function we developed in the previous section.\n",
    "# Once loaded, we can split the data into separate training sequences by splitting based on new lines.\n",
    "\n",
    "# load doc(republic_sequences.txt) into memory\n",
    "in_filename = 'C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction and analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions', 'and analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of', 'analysis the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the', 'the republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state', 'republic of plato is the longest of his works with the exception of the laws and is certainly the greatest of them there are nearer approaches to modern metaphysics in the philebus and in the sophist the politicus or statesman is more ideal the form and institutions of the state are']\n"
     ]
    }
   ],
   "source": [
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The word embedding layer expects input sequences to be comprised of integers.\n",
    "# We can map each word in our vocabulary to a unique integer and encode our input sequences. \n",
    "# Later, when we make predictions, we can convert the prediction to numbers and look up their \n",
    "# associated words in the same mapping.\n",
    "# To do this encoding, we will use the Tokenizer class in the Keras API.\n",
    "# First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the \n",
    "# unique words in the data and assigns each a unique integer.\n",
    "# We can then use the fit Tokenizer to encode all of the training sequences, \n",
    "# converting each sequence from a list of words to a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can access the mapping of words to integers as a dictionary attribute called word_index on the Tokenizer object.\n",
    "# We need to know the size of the vocabulary for defining the embedding layer later. \n",
    "# We can determine the vocabulary by calculating the size of the mapping dictionary.\n",
    "# Words are assigned values from 1 to the total number of words (e.g. 7,409). \n",
    "# The Embedding layer needs to allocate a vector representation for each word in this vocabulary \n",
    "# from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the \n",
    "# word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length.\n",
    "# Therefore, when specifying the vocabulary size to the Embedding layer, we specify it as 1 larger than\n",
    "# the actual vocabulary.\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Inputs and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.\n",
    "# We can do this with array slicing.\n",
    "# After separating, we need to one hot encode the output word. \n",
    "# This means converting it from an integer to a vector of 0 values, one for each \n",
    "# word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.\n",
    "# This is so that the model learns to predict the probability distribution for the next word and the \n",
    "# ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
    "# Keras provides the to_categorical() that can be used to one hot encode the output words for \n",
    "# each input-output sequence pair.\n",
    "\n",
    "# Finally, we need to specify to the Embedding layer how long input sequences are. \n",
    "# We know that there are 50 words because we designed the model, but a good generic way to specify that is to use the second dimension \n",
    "# (number of columns) of the input data’s shape. \n",
    "# That way, if you change the length of sequences when preparing data, \n",
    "# you do not need to change this data loading code; it is generic.\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            520500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10410)             1051410   \n",
      "=================================================================\n",
      "Total params: 1,722,810\n",
      "Trainable params: 1,722,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "214919/214919 [==============================] - 890s 4ms/step - loss: 5.3310 - acc: 0.1642\n",
      "Epoch 2/2\n",
      "214919/214919 [==============================] - 795s 4ms/step - loss: 5.1947 - acc: 0.1679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f1073b1898>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to time and resources constraints on my laptop, epochs kepts only 2. And hence obviously accuracy is low.\n",
    "# There are many techniques you can try to boost the accuracy of this model\n",
    "\n",
    "# Try tuning following parameters:\n",
    "\n",
    "# 1) learning rate - alpha\n",
    "# 2) Beta - Momentum\n",
    "# 3)Optimizer - beta1, beta2, error \n",
    "# 4) increase # of layers\n",
    "# 5) increase # of hidden nodes\n",
    "# 6) learning rate decay\n",
    "# 7) Mini batch size\n",
    "# 8) increase number of epochs\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be to compel the best minds to attain that knowledge which we have already shown to be the greatest of must continue to ascend until they arrive at the good but when they have ascended and seen enough we must not allow them to do as they do now what do\n",
      "\n",
      "that the same and the\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('C:\\\\Users\\\\G560042\\\\OneDrive - General Mills\\\\Desktop\\\\Rohan\\\\#Extra Projects\\\\Deep Learning\\\\projects\\\\Word Generation - NLP\\\\tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 5)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
